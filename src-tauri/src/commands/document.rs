use crate::app_state::AppState;
use crate::db::models::Document;
use crate::rag::text_splitter::TextSplitter;
use crate::rag::vector_store::VectorDocument;
use crate::commands::file::read_file_content;
use tauri::State;
use uuid::Uuid;
use reqwest::Client;
use std::path::Path;

#[derive(serde::Deserialize)]
pub struct UploadDocumentRequest {
    name: String,
    content: String,
    file_type: Option<String>,
}

#[derive(serde::Serialize)]
pub struct UploadDocumentResponse {
    success: bool,
    message: String,
    document_id: Option<String>,
}

/// ä¸Šä¼ æ–‡æ¡£
#[tauri::command]
pub async fn upload_document(
    request: UploadDocumentRequest,
    state: State<'_, AppState>,
) -> Result<UploadDocumentResponse, String> {
    // æ£€æŸ¥ RAG æœåŠ¡æ˜¯å¦å·²åˆå§‹åŒ–
    if !state.is_rag_initialized() {
        return Ok(UploadDocumentResponse {
            success: false,
            message: "è¯·å…ˆé…ç½® API Key".to_string(),
            document_id: None,
        });
    }
    
    let document_id = Uuid::new_v4().to_string();
    let timestamp = chrono::Utc::now().timestamp();
    
    // 1. ä¿å­˜æ–‡æ¡£åˆ°æ•°æ®åº“
    let file_size = request.content.len() as i64;
    
    sqlx::query(
        "INSERT INTO documents (id, name, content, file_type, file_size, created_at, updated_at) 
         VALUES (?, ?, ?, ?, ?, ?, ?)"
    )
    .bind(&document_id)
    .bind(&request.name)
    .bind(&request.content)
    .bind(&request.file_type)
    .bind(file_size)
    .bind(timestamp)
    .bind(timestamp)
    .execute(state.db.pool())
    .await
    .map_err(|e| e.to_string())?;
    
    // 2. æ–‡æœ¬åˆ†å—
    let chunks = {
        let config = state.rag_config.lock().unwrap();
        let splitter = TextSplitter::new(config.chunk_size, config.chunk_overlap);
        splitter.split_smart(&request.content)
    }; // config çš„ MutexGuard åœ¨è¿™é‡Œè‡ªåŠ¨é‡Šæ”¾
    
    let start_time = std::time::Instant::now();
    println!("ğŸ“¦ æ–‡æ¡£åˆ†å—å®Œæˆ: {} ä¸ªå—", chunks.len());
    
    // 3. æ‰¹é‡ç”Ÿæˆå‘é‡ï¼ˆæ¯æ‰¹ 25 ä¸ª - é€šä¹‰åƒé—® API é™åˆ¶ï¼‰
    const BATCH_SIZE: usize = 25;
    let total_chunks = chunks.len();
    let mut all_embeddings = Vec::new();
    
    // è·å– API é…ç½®
    let (api_key, model) = {
        let guard = state.embedding_service.lock().unwrap();
        let service = guard.as_ref().ok_or("EmbeddingæœåŠ¡æœªåˆå§‹åŒ–")?;
        (service.api_key().clone(), service.model().clone())
    };
    
    // åˆ›å»º HTTP å®¢æˆ·ç«¯
    let client = Client::builder()
        .timeout(std::time::Duration::from_secs(60))
        .build()
        .map_err(|e| format!("åˆ›å»ºHTTPå®¢æˆ·ç«¯å¤±è´¥: {}", e))?;
    
    // å¹¶å‘å¤„ç†å¤šä¸ªæ‰¹æ¬¡ï¼ˆæœ€å¤š 5 ä¸ªå¹¶å‘ï¼‰
    let mut batch_tasks = Vec::new();
    
    for (batch_idx, chunk_batch) in chunks.chunks(BATCH_SIZE).enumerate() {
        let client_clone = client.clone();
        let api_key_clone = api_key.clone();
        let model_clone = model.clone();
        let batch_texts: Vec<String> = chunk_batch.iter().map(|s| s.to_string()).collect();
        let batch_num = batch_idx + 1;
        let total_batches = (total_chunks + BATCH_SIZE - 1) / BATCH_SIZE;
        
        let task = tokio::spawn(async move {
            println!("ğŸš€ æ­£åœ¨å¤„ç†æ‰¹æ¬¡ {}/{} ({} ä¸ªå—)...", batch_num, total_batches, batch_texts.len());
            
            let request_body = serde_json::json!({
                "model": model_clone,
                "input": {
                    "texts": batch_texts
                }
            });
            
            let response = client_clone
                .post("https://dashscope.aliyuncs.com/api/v1/services/embeddings/text-embedding/text-embedding")
                .header("Authorization", format!("Bearer {}", api_key_clone))
                .header("Content-Type", "application/json")
                .json(&request_body)
                .send()
                .await
                .map_err(|e| format!("æ‰¹æ¬¡ {} Embedding APIç½‘ç»œè¯·æ±‚å¤±è´¥: {}", batch_num, e))?;
            
            if !response.status().is_success() {
                let status = response.status();
                let text = response.text().await.map_err(|e| e.to_string())?;
                return Err(format!("æ‰¹æ¬¡ {} Embedding API è¯·æ±‚å¤±è´¥: {} - {}", batch_num, status, text));
            }
            
            let result: serde_json::Value = response.json().await
                .map_err(|e| format!("æ‰¹æ¬¡ {} è§£æå“åº”å¤±è´¥: {}", batch_num, e))?;
            
            let embeddings_array = result["output"]["embeddings"].as_array()
                .ok_or_else(|| format!("æ‰¹æ¬¡ {} API è¿”å›æ ¼å¼é”™è¯¯", batch_num))?;
            
            let mut batch_embeddings = Vec::new();
            for emb in embeddings_array {
                let embedding = emb["embedding"].as_array()
                    .ok_or_else(|| format!("æ‰¹æ¬¡ {} å‘é‡æ•°æ®æ ¼å¼é”™è¯¯", batch_num))?
                    .iter()
                    .filter_map(|v| v.as_f64().map(|f| f as f32))
                    .collect::<Vec<f32>>();
                
                if embedding.is_empty() {
                    return Err(format!("æ‰¹æ¬¡ {} å‘é‡ä¸ºç©º", batch_num));
                }
                
                batch_embeddings.push(embedding);
            }
            
            println!("âœ… æ‰¹æ¬¡ {}/{} å®Œæˆ", batch_num, total_batches);
            Ok::<Vec<Vec<f32>>, String>(batch_embeddings)
        });
        
        batch_tasks.push(task);
        
        // æ§åˆ¶å¹¶å‘æ•°ï¼šæ¯ 10 ä¸ªä»»åŠ¡ç­‰å¾…ä¸€æ‰¹å®Œæˆ
        if batch_tasks.len() >= 10 {
            let completed = batch_tasks.remove(0);
            let embeddings = completed.await
                .map_err(|e| format!("ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {}", e))?
                .map_err(|e| e)?;
            all_embeddings.extend(embeddings);
        }
    }
    
    // ç­‰å¾…å‰©ä½™ä»»åŠ¡å®Œæˆ
    for task in batch_tasks {
        let embeddings = task.await
            .map_err(|e| format!("ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {}", e))?
            .map_err(|e| e)?;
        all_embeddings.extend(embeddings);
    }
    
    let embedding_time = start_time.elapsed();
    println!("ğŸ‰ æ‰€æœ‰å‘é‡ç”Ÿæˆå®Œæˆ: {} ä¸ª (è€—æ—¶: {:.2}ç§’)", all_embeddings.len(), embedding_time.as_secs_f64());
    
    // 4. æ‰¹é‡ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆå•ä¸ªäº‹åŠ¡ï¼‰
    println!("ğŸ’¾ å¼€å§‹æ‰¹é‡ä¿å­˜åˆ°æ•°æ®åº“...");
    let mut tx = state.db.pool().begin().await
        .map_err(|e| format!("å¼€å§‹äº‹åŠ¡å¤±è´¥: {}", e))?;
    
    for (index, (chunk_content, embedding)) in chunks.iter().zip(all_embeddings.iter()).enumerate() {
        let chunk_id = Uuid::new_v4().to_string();
        
        // ä¿å­˜ chunk åˆ°æ•°æ®åº“
        sqlx::query(
            "INSERT INTO chunks (id, document_id, content, chunk_index, created_at) 
             VALUES (?, ?, ?, ?, ?)"
        )
        .bind(&chunk_id)
        .bind(&document_id)
        .bind(chunk_content)
        .bind(index as i64)
        .bind(timestamp)
        .execute(&mut *tx)
        .await
        .map_err(|e| format!("æ’å…¥å— {} å¤±è´¥: {}", index, e))?;
        
        // æ·»åŠ åˆ°å‘é‡å­˜å‚¨
        let vector_doc = VectorDocument {
            id: chunk_id.clone(),
            content: chunk_content.clone(),
            embedding: embedding.clone(),
            metadata: serde_json::json!({
                "document_id": document_id,
                "chunk_index": index,
                "document_name": request.name,
            }),
        };
        
        state.vector_store.lock().unwrap().add_document(vector_doc);
    }
    
    // æäº¤äº‹åŠ¡
    tx.commit().await
        .map_err(|e| format!("æäº¤äº‹åŠ¡å¤±è´¥: {}", e))?;
    
    let total_time = start_time.elapsed();
    println!("âœ… æ•°æ®åº“ä¿å­˜å®Œæˆ (æ€»è€—æ—¶: {:.2}ç§’)", total_time.as_secs_f64());
    
    Ok(UploadDocumentResponse {
        success: true,
        message: format!("æ–‡æ¡£ä¸Šä¼ æˆåŠŸï¼Œå…±åˆ†ä¸º {} ä¸ªå—", chunks.len()),
        document_id: Some(document_id),
    })
}

/// è·å–æ‰€æœ‰æ–‡æ¡£åˆ—è¡¨
#[tauri::command]
pub async fn get_documents(
    state: State<'_, AppState>,
) -> Result<Vec<Document>, String> {
    let documents = sqlx::query_as::<_, Document>(
        "SELECT * FROM documents ORDER BY created_at DESC"
    )
    .fetch_all(state.db.pool())
    .await
    .map_err(|e| e.to_string())?;
    
    Ok(documents)
}

/// åˆ é™¤æ–‡æ¡£
#[tauri::command]
pub async fn delete_document(
    document_id: String,
    state: State<'_, AppState>,
) -> Result<bool, String> {
    // ä»æ•°æ®åº“åˆ é™¤ï¼ˆä¼šçº§è”åˆ é™¤chunksï¼‰
    sqlx::query("DELETE FROM documents WHERE id = ?")
        .bind(&document_id)
        .execute(state.db.pool())
        .await
        .map_err(|e| e.to_string())?;
    
    // ä»å‘é‡å­˜å‚¨åˆ é™¤
    state.vector_store.lock().unwrap().remove_by_document_id(&document_id);
    
    Ok(true)
}

/// ä»æ–‡ä»¶è·¯å¾„ä¸Šä¼ æ–‡æ¡£ï¼ˆæ”¯æŒ PDF ç­‰æ ¼å¼ï¼‰
#[tauri::command]
pub async fn upload_document_from_path(
    file_path: String,
    state: State<'_, AppState>,
) -> Result<UploadDocumentResponse, String> {
    // æ£€æŸ¥ RAG æœåŠ¡æ˜¯å¦å·²åˆå§‹åŒ–
    if !state.is_rag_initialized() {
        return Ok(UploadDocumentResponse {
            success: false,
            message: "è¯·å…ˆé…ç½® API Key".to_string(),
            document_id: None,
        });
    }
    
    // è·å–æ–‡ä»¶å
    let file_name = Path::new(&file_path)
        .file_name()
        .and_then(|n| n.to_str())
        .unwrap_or("æœªçŸ¥æ–‡ä»¶")
        .to_string();
    
    // æ£€æµ‹æ–‡ä»¶ç±»å‹
    let file_type = Path::new(&file_path)
        .extension()
        .and_then(|e| e.to_str())
        .unwrap_or("unknown")
        .to_string();
    
    // è¯»å–æ–‡ä»¶å†…å®¹ï¼ˆä¼šè‡ªåŠ¨å¤„ç† PDFï¼‰
    let content = read_file_content(file_path).await?;
    
    // è°ƒç”¨åŸæœ‰çš„ä¸Šä¼ é€»è¾‘
    let request = UploadDocumentRequest {
        name: file_name,
        content,
        file_type: Some(file_type),
    };
    
    upload_document(request, state).await
}

